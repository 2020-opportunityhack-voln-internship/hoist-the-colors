{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Waw4M3na3AmO","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uguFM1QO0PU4","colab_type":"code","colab":{}},"source":["import os\n","os.listdir('.')\n","os.getcwd()\n","os.chdir('/content/drive/My Drive/Colab Notebooks')\n","os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jt1cvmDaZAVn","colab_type":"code","colab":{}},"source":["pip install tensorflow_gpu==2.2.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ui21Mu6QhZcP","colab_type":"code","colab":{}},"source":["pip install keras==2.4.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WUJlQJCWY9Lg","colab_type":"code","colab":{}},"source":["pip install tensorflow==2.2.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KreJkrkPiEB9","colab_type":"code","colab":{}},"source":["pip show tensorflow\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-sQWkCkWZ-j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595916376553,"user_tz":420,"elapsed":370,"user":{"displayName":"Karishma Joseph","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKjBvkSwmhJalHEtYBS-DpZ3PtCyHQAkDlCa5QCA=s64","userId":"03647980348141807206"}},"outputId":"c5a7fe04-8b02-4f26-c2b8-3aeb8cec50b3"},"source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","# from nltk.stem import SnowballStemmer\n","from textblob import TextBlob\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","from keras.layers import Dense, Input, Embedding, Lambda, Dropout, SpatialDropout1D, GlobalAveragePooling1D, merge, Flatten, Bidirectional, GRU, GlobalMaxPooling1D\n","from keras.layers.merge import concatenate\n","from keras.models import Model\n","from keras import optimizers\n","from keras import initializers\n","from keras.engine import InputSpec, Layer\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.metrics import roc_auc_score\n","from sklearn.externals import joblib\n","from sklearn.model_selection import KFold"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8I2TswM23XWl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595915844266,"user_tz":420,"elapsed":1863,"user":{"displayName":"Karishma Joseph","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKjBvkSwmhJalHEtYBS-DpZ3PtCyHQAkDlCa5QCA=s64","userId":"03647980348141807206"}}},"source":["glove_embedding_file = '/content/drive/My Drive/Colab Notebooks/glove.840B.300d.txt'\n","train_data_file = '/content/drive/My Drive/Colab Notebooks/train.csv'\n","test_data_file = '/content/drive/My Drive/Colab Notebooks/test.csv'\n","\n","max_sequence_length = 400\n","max_nb_words = 100000\n","embedding_size = 300\n","\n","train_data = pd.read_csv(train_data_file)\n","test_data = pd.read_csv(test_data_file)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f1tUEUPW4dgq","colab_type":"text"},"source":["# **Create embedding index**"]},{"cell_type":"code","metadata":{"id":"2_IVutHj3msb","colab_type":"code","colab":{}},"source":["def create_embedding_index(path):\n","    embeddings_index = {}\n","    with open(path, 'r', encoding='utf-8') as f:\n","      for line in f:\n","          words = line.split()\n","          try:\n","              word = words[0]\n","              values = np.asarray(words[1:], dtype='float32')\n","              embeddings_index[word] = values\n","          except:\n","              continue\n","\n","    return embeddings_index\n","\n","embeddings_index = create_embedding_index(glove_embedding_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jwvCG8pK4xW-","colab_type":"text"},"source":["# **Clean data**"]},{"cell_type":"code","metadata":{"id":"ObledVIf4p8l","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595915853137,"user_tz":420,"elapsed":370,"user":{"displayName":"Karishma Joseph","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKjBvkSwmhJalHEtYBS-DpZ3PtCyHQAkDlCa5QCA=s64","userId":"03647980348141807206"}}},"source":["def preprocess_text(text):\n","    # convert to lower case\n","    text = text.lower()\n","\n","    #remove links and numbers\n","    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n","    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n","\n","    #spelling check (Takes a really long time)\n","    # text = str(TextBlob(text).correct())\n","    \n","    #abbreviations \n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\?\", \" ? \", text)\n","    text = re.sub(r\"\\!\", \" ! \", text)\n","    text = re.sub(r\"\\\"\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","\n","    #remove special characters\n","    text = re.sub(r'[^?!.,:a-z\\d ]', '',text, flags=re.IGNORECASE)\n","    \n","    #stop word removal\n","    STOPWORDS = set(stopwords.words('english'))\n","    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n","\n","    return text"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXaRaZEW-uFn","colab_type":"text"},"source":["# **Process Text in dataset**"]},{"cell_type":"code","metadata":{"id":"o4ejfk4D9_ft","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595915896637,"user_tz":420,"elapsed":40579,"user":{"displayName":"Karishma Joseph","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKjBvkSwmhJalHEtYBS-DpZ3PtCyHQAkDlCa5QCA=s64","userId":"03647980348141807206"}}},"source":["train_sentences = train_data[\"comment_text\"].fillna(\"no comment\").values\n","test_sentences = test_data[\"comment_text\"].fillna(\"no comment\").values\n","classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n","y = train_data[classes].values\n","\n","train_sentences = [preprocess_text(text) for text in train_sentences]    \n","test_sentences=[preprocess_text(text) for text in test_sentences]\n","\n","tokenizer = Tokenizer(num_words=max_nb_words, filters='\"#%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')\n","tokenizer.fit_on_texts(train_sentences + test_sentences)\n","\n","#save tokenizer\n","joblib_file = \"Tokenizer.pkl\"  \n","joblib.dump(tokenizer, joblib_file)\n","\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","test_sequences = tokenizer.texts_to_sequences(test_sentences)\n","\n","word_index = tokenizer.word_index\n","train_data = pad_sequences(train_sequences, maxlen=max_sequence_length)\n","test_data = pad_sequences(test_sequences, maxlen=max_sequence_length)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zj6eKdWX-_St","colab_type":"text"},"source":["## **Prepare embedding matrix**"]},{"cell_type":"code","metadata":{"id":"Zp-7_Dod_Cm0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595915945213,"user_tz":420,"elapsed":928,"user":{"displayName":"Karishma Joseph","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKjBvkSwmhJalHEtYBS-DpZ3PtCyHQAkDlCa5QCA=s64","userId":"03647980348141807206"}}},"source":["nb_words = len(word_index) + 1\n","embedding_matrix = np.zeros((nb_words, embedding_size))\n","\n","for word, i in word_index.items():\n","    if i >= max_nb_words:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XecFuls3_Lle","colab_type":"text"},"source":["# **Bidirectional Recurrent Neural Network**"]},{"cell_type":"code","metadata":{"id":"xoWFChQ3_OLM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595915948061,"user_tz":420,"elapsed":382,"user":{"displayName":"Karishma Joseph","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKjBvkSwmhJalHEtYBS-DpZ3PtCyHQAkDlCa5QCA=s64","userId":"03647980348141807206"}}},"source":["def rnn(nb_words, embedding_size, embedding_matrix, max_sequence_length, out_size):\n","    recurrent_units = 60\n","    input_layer = Input(shape=(max_sequence_length,))\n","    embedding_layer = Embedding(nb_words,\n","                                embedding_size,\n","                                weights=[embedding_matrix],\n","                                input_length=max_sequence_length,\n","                                trainable=False)(input_layer)\n","    embedding_layer = SpatialDropout1D(0.25)(embedding_layer)\n","\n","    #CuDNNGRU\n","    rnn_layer_1 = Bidirectional(GRU(recurrent_units, return_sequences=True))(embedding_layer)\n","    rnn_layer_2 = Bidirectional(GRU(recurrent_units, return_sequences=True))(rnn_layer_1)\n","    x = concatenate([rnn_layer_1, rnn_layer_2], axis=2)\n","\n","    last = Lambda(lambda t: t[:, -1], name='last')(x)\n","    maxpool = GlobalMaxPooling1D()(x)\n","    average = GlobalAveragePooling1D()(x)\n","\n","    concatenated_layer = concatenate([last, maxpool, average], axis=1)\n","    x = Dropout(0.5)(concatenated_layer)\n","    x = Dense(144, activation=\"relu\")(x)\n","    output_layer = Dense(out_size, activation=\"sigmoid\")(x)\n","    model = Model(inputs=input_layer, outputs=output_layer)\n","    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n","    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n","    model.summary()\n","    return model"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4wclKJakA-G5","colab_type":"text"},"source":["# **Train Model**"]},{"cell_type":"code","metadata":{"id":"bLwGTa5uBCPK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595916731405,"user_tz":420,"elapsed":430,"user":{"displayName":"Karishma Joseph","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKjBvkSwmhJalHEtYBS-DpZ3PtCyHQAkDlCa5QCA=s64","userId":"03647980348141807206"}}},"source":["class RNNModel(object):\n","\n","    def __init__(self, model_stamp, epoch_num, learning_rate):\n","        self.models = []\n","        self.epoch_num = epoch_num\n","        self.learning_rate = learning_rate\n","        self.model_stamp = model_stamp\n","        self.val_loss = -1\n","        self.auc = -1\n","\n","    \n","    def train_k_folds(self, X, y, n_folds, batch_size, get_model_func):\n","        models = []\n","        fold_predictions = []\n","        score = 0\n","        total_auc = 0\n","\n","        #k-fold cross validation\n","        kf = KFold(n_splits=n_folds, random_state=None) \n","\n","        for fold_id, (train_index, val_index) in enumerate(kf.split(X)):\n","            print(\"Train:\", train_index, \"Validation:\",val_index)\n","            train_x, val_x = X[train_index], X[val_index] \n","            train_y, val_y = y[train_index], y[val_index]\n","\n","            model, bst_val_loss, fold_prediction, auc = self._train_model(\n","              get_model_func(), batch_size, train_x, train_y, val_x, val_y, fold_id)\n","            \n","            total_val_loss += bst_val_loss\n","            total_auc += auc\n","            models.append(model)\n","            fold_predictions.append(fold_prediction)\n","\n","        self.models = models\n","        self.val_loss = total_val_loss / n_folds\n","        self.auc = total_auc / n_folds\n","        return models, self.val_loss, self.auc, fold_predictions\n","\n","    def _train_model(self, model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n","        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2)\n","        model_path = self.model_stamp + str(fold_id) + '.h5'\n","        model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, save_weights_only=True)\n","        hist = model.fit(train_x, train_y,\n","                         validation_data=(val_x, val_y),\n","                         epochs=self.epoch_num, batch_size=batch_size, shuffle=True,\n","                         callbacks=[early_stopping, model_checkpoint])\n","        best_val_score = min(hist.history['val_loss'])\n","        print(\"Validation score\", best_val_score)\n","        predictions = model.predict(val_x)\n","        auc = roc_auc_score(val_y, predictions)\n","        print(\"AUC Score\", auc)\n","        return model, best_val_score, predictions, auc"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9TI0bhqBIDS","colab_type":"code","colab":{}},"source":["def rnn_model():\n","    return rnn(nb_words, embedding_size, embedding_matrix, max_sequence_length, out_size=6)\n","\n","model = RNNModel(model_stamp='kmax_text_rnn', epoch_num=50, learning_rate=1e-3)\n","trained_models, val_loss, auc, fold_predictions = model.train_k_folds(train_data, y, n_folds=3, batch_size=256, get_model_func=rnn_model)\n","\n","print(\"Overall val-loss:\", val_loss, \"AUC\", auc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UjtAn1m2vBFi","colab_type":"text"},"source":["# **Save Model**"]},{"cell_type":"code","metadata":{"id":"LaNyNym7pubq","colab_type":"code","colab":{}},"source":["#choosing the best model\n","model = trained_models[1]\n","\n","#save model\n","model.save('RNN_Model.h5')\n","model = load_model('RNN_Model.h5')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nKoiNNGgDmul","colab_type":"text"},"source":["# **Test Model**"]},{"cell_type":"code","metadata":{"id":"5B5Kanap4Ad3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595547453372,"user_tz":420,"elapsed":17986,"user":{"displayName":"Karishma Joseph","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKjBvkSwmhJalHEtYBS-DpZ3PtCyHQAkDlCa5QCA=s64","userId":"03647980348141807206"}},"outputId":"fa4c0833-226c-4b2d-d039-8a72af85e50b"},"source":["from sklearn.externals import joblib\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","import os\n","\n","# test_sentences = [\"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\", '== From RfC == \\n\\n The title is fine as it is, IMO.', '\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"', \":If you have a look back at the source, the information I updated was the correct form. I can only guess the source hadn't updated. I shall update the information once again but thank you for your message.\", \"I don't anonymously edit articles at all.\", 'Thank you for understanding. I think very highly of you and would not revert without discussion.', 'Please do not add nonsense to Wikipedia. Such edits are considered vandalism and quickly undone. If you would like to experiment, please use the sandbox instead. Thank you.   -', ':Dear god this site is horrible.', '\" \\n Only a fool can believe in such numbers. \\n The correct number lies between 10 000 to 15 000. \\n Ponder the numbers carefully.  \\n\\n This error will persist for a long time as it continues to reproduce... The latest reproduction I know is from ENCYCLOPÆDIA BRITANNICA ALMANAC 2008 wich states \\n Magnittude: 8.7 (fair enough) \\n victims: 70 000 (today 10 000 to 15 000 is not \"\"a lot\"\" so I guess people just come out with a number that impresses enough, I don\\'t know. But I know this: it\\'s just a shameless lucky number that they throw in the air. \\n GC \\n\\n \"', \"== Double Redirects == \\n\\n When fixing double redirects, don't just blank the outer one, you need edit it to point it to the final target, unless you think it's inappropriate, in which case, it needs to be nominated at WP:RfD\", 'I think its crap that the link to roggenbier is to this article. Somebody that knows how to do things should change it.', '\"::: Somebody will invariably try to add Religion?  Really??  You mean, the way people have invariably kept adding \"\"Religion\"\" to the Samuel Beckett infobox?  And why do you bother bringing up the long-dead completely non-existent \"\"Influences\"\" issue?  You\\'re just flailing, making up crap on the fly. \\n ::: For comparison, the only explicit acknowledgement in the entire Amos Oz article that he is personally Jewish is in the categories!    \\n\\n \"', \", 25 February 2010 (UTC) \\n\\n :::Looking it over, it's clear that  (a banned sockpuppet of ) ignored the consensus (&, fwiw, policy-appropriate) choice to leave the page at Chihuahua (Mexico) and the current page should be returned there. Anyone have the time to fix the incoming links? -  18:24\", '\" \\n\\n It says it right there that it IS a type. The \"\"Type\"\" of institution is needed in this case because there are three levels of SUNY schools: \\n -University Centers and Doctoral Granting Institutions \\n -State Colleges \\n -Community Colleges. \\n\\n It is needed in this case to clarify that UB is a SUNY Center. It says it even in Binghamton University, University at Albany, State University of New York, and Stony Brook University. Stop trying to say it\\'s not because I am totally right in this case.\"', '\" \\n\\n == Before adding a new product to the list, make sure it\\'s relevant == \\n\\n Before adding a new product to the list, make sure it has a wikipedia entry already, \"\"proving\"\" it\\'s relevance and giving the reader the possibility to read more about it. \\n Otherwise it could be subject to deletion. See this article\\'s revision history.\"', '==Current Position== \\n Anyone have confirmation that Sir, Alfred is no longer at the airport and is hospitalised?', 'this other one from 1897', '== Reason for banning throwing == \\n\\n This article needs a section on /why/ throwing is banned. At the moment, to a non-cricket fan, it seems kind of arbitrary.', \":: Wallamoose was changing the cited material to say things the original source did not say. In response to his objections, I modified the article as we went along. I was not just reverting him. I repeatedly asked him to use the talk page. I've been trying to add to the article for a long time.  It's so thin on content. This is wrong.\", '|blocked]] from editing Wikipedia.   |']\n","\n","CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n","test_sentences = [\"Go back to your country\"]\n","tokenizer = joblib.load('Tokenizer.pkl')\n","test_sequences = tokenizer.texts_to_sequences(test_sentences)\n","test_data = pad_sequences(test_sequences, maxlen=400)\n","model = load_model('RNN_Model.h5')\n","test_predicts = model.predict(test_data, batch_size=256, verbose=1)\n","print(test_predicts)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1/1 [==============================] - 0s 24ms/step\n","[[0.14269954 0.0015585  0.01939207 0.01231543 0.05094578 0.00594961]]\n"],"name":"stdout"}]}]}